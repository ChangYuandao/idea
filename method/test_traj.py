"""
æµ‹è¯•åå¥½å­¦ä¹ æµç¨‹çš„ç‹¬ç«‹è„šæœ¬
ç”¨äºæµ‹è¯•ä»åŠ è½½è½¨è¿¹åˆ°æ›´æ–°å¥–åŠ±å‡½æ•°å‚æ•°çš„å®Œæ•´æµç¨‹
"""
import os
import sys
import logging
from pathlib import Path
import traceback

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
EUREKA_ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, EUREKA_ROOT_DIR)

from utils.preference_learning import IsaacGymPreferenceLearning
from utils.reward_parser import extract_reward_parameters
from utils.reward_updater import update_reward_function_with_params

# é…ç½®æ—¥å¿—ï¼šæ–°å¢å°†æ—¥å¿—å†™å…¥æ–‡ä»¶ test_run.log
log_file = Path(EUREKA_ROOT_DIR) / "test_run.log"

logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s][%(name)s][%(levelname)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.StreamHandler(sys.stdout),            # ä¿ç•™åŸæ¥çš„ç»ˆç«¯è¾“å‡º
        logging.FileHandler(log_file, mode='w', encoding='utf-8')  # æ–°å¢ï¼šè¾“å‡ºåˆ°æ–‡ä»¶
    ]
)

logging.info(f"æ—¥å¿—å°†ä¿å­˜åˆ°: {log_file}")

def test_preference_learning(
    trajectory_file: str,
    reward_file: str,
    evaluate_dir: str,
    task_name: str = "Ant",
    min_consecutive_steps: int = 5,
    beta: float = 1.0
):
    """
    æµ‹è¯•åå¥½å­¦ä¹ æµç¨‹
    
    Args:
        trajectory_file: è½¨è¿¹æ–‡ä»¶è·¯å¾„ (.pkl)
        reward_file: å¥–åŠ±å‡½æ•°æ–‡ä»¶è·¯å¾„ (_rewardonly.py)
        evaluate_dir: è¯„ä¼°å‡½æ•°ç›®å½•è·¯å¾„
        task_name: ä»»åŠ¡åç§°
        min_consecutive_steps: æœ€å°è¿ç»­åå¥½æ­¥æ•°
        beta: ç†æ€§ç³»æ•°
    """
    logging.info("="*80)
    logging.info("Starting Preference Learning Test")
    logging.info("="*80)
    
    # 1. éªŒè¯æ–‡ä»¶å­˜åœ¨æ€§
    logging.info("[Step 1] Validating input files...")
    
    trajectory_path = Path(trajectory_file)
    reward_path = Path(reward_file)
    evaluate_path = Path(evaluate_dir)
    
    if not trajectory_path.exists():
        logging.error(f"âŒ Trajectory file not found: {trajectory_path}")
        return False
    logging.info(f"âœ… Trajectory file found: {trajectory_path}")
    
    if not reward_path.exists():
        logging.error(f"âŒ Reward file not found: {reward_path}")
        return False
    logging.info(f"âœ… Reward file found: {reward_path}")
    
    if not evaluate_path.exists():
        logging.warning(f"âš ï¸ Evaluate directory not found: {evaluate_path}")
        logging.warning("Will use random preference generation")
    else:
        logging.info(f"âœ… Evaluate directory found: {evaluate_path}")
    
    try:
        # 2. æå–å¥–åŠ±å‡½æ•°å‚æ•°
        logging.info("[Step 2] Extracting reward function parameters...")
        hp_ranges, initial_values = extract_reward_parameters(str(reward_path))
        
        # 3. åˆå§‹åŒ–åå¥½å­¦ä¹ å™¨
        logging.info("[Step 3] Initializing preference learner...")
        preference_learner = IsaacGymPreferenceLearning(
            hp_ranges=hp_ranges,
            initial_values=initial_values,
            reward_file_path=str(reward_path),
            beta=beta,
            task_name=task_name
        )
        logging.info("âœ… Preference learner initialized")
        
        # 4. åŠ è½½è½¨è¿¹
        logging.info("[Step 4] Loading trajectories...")
        trajectories = preference_learner.load_trajectories(str(trajectory_path))
        logging.info(f"âœ… Loaded {len(trajectories)} trajectories")
        
        # æ‰“å°ç¬¬ä¸€æ¡è½¨è¿¹çš„ä¿¡æ¯
        if len(trajectories) > 0:
            logging.info(f"First trajectory keys: {list(trajectories[0].keys())}")
            for key, value in trajectories[0].items():
                if hasattr(value, 'shape'):
                    logging.info(f"  {key}: shape={value.shape}, dtype={value.dtype}")
                else:
                    logging.info(f"  {key}: type={type(value)}")
        
        # 5. ç”Ÿæˆåå¥½å¯¹
        logging.info("[Step 5] Generating preference pairs...")
        
        if evaluate_path.exists():
            # ä½¿ç”¨è¯„ä¼°å‡½æ•°ç”Ÿæˆåå¥½å¯¹
            logging.info("Using evaluation functions to generate preferences...")
            preferences = preference_learner.generate_preference_buffer(
                trajectories,
                str(evaluate_path),
                min_consecutive=min_consecutive_steps
            )

        
        if len(preferences) == 0:
            logging.warning("âš ï¸ No preferences generated!")
            return False
        
        logging.info(f"âœ… Generated {len(preferences)} preference pairs")
        
        # 6. æ›´æ–°å¥–åŠ±å‡½æ•°å‚æ•°
        logging.info("[Step 6] Updating reward function parameters...")
        updated_params = preference_learner.update_reward_parameters(
            trajectories,
            preferences,
        )
        
        logging.info("âœ… Parameters updated successfully!")
        logging.info("Parameter changes:")
        for param_name in hp_ranges.keys():
            old_val = initial_values.get(param_name, 0)
            new_val = updated_params.get(param_name, 0)
            change = new_val - old_val
            change_pct = (change / old_val * 100) if old_val != 0 else 0
            logging.info(f"  {param_name}: {old_val:.4f} -> {new_val:.4f} ({change:+.4f}, {change_pct:+.2f}%)")
        
        # 7. æ›´æ–°å¥–åŠ±å‡½æ•°ä»£ç 
        logging.info("[Step 7] Updating reward function code...")
        output_dir = Path(EUREKA_ROOT_DIR) / "test_outputs"
        output_dir.mkdir(exist_ok=True)
        
        updated_code_path = update_reward_function_with_params(
            original_code_path=str(reward_path),
            updated_params=updated_params,
            output_path=output_dir / "updated_reward_function.py"
        )
        
        logging.info(f"âœ… Updated reward function saved to: {updated_code_path}")
        
        # 8. æµ‹è¯•å®Œæˆ
        logging.info("\n" + "="*80)
        logging.info("ğŸ‰ Preference Learning Test PASSED!")
        logging.info("="*80)
        
        return True
        
    except Exception as e:
        logging.error(f"\nâŒ Test FAILED with error: {e}")
        logging.error(traceback.format_exc())
        return False


def main():


    # è¿è¡Œæµ‹è¯•
    test_preference_learning(
        trajectory_file="/home/changyuandao/changyuandao/paperProject/idea/method/outputs/shadow_hand/iter0_trajectories.pkl",
        reward_file="/home/changyuandao/changyuandao/paperProject/idea/method/outputs/shadow_hand/env_iter0_role_CONSERVATOR_response0_rewardonly.py",
        evaluate_dir="/home/changyuandao/changyuandao/paperProject/idea/method/utils/prompts/evaluate_function/ShadowHand",
        task_name="ShadowHand",
        min_consecutive_steps=5,
        beta=1.0
    )
    


if __name__ == "__main__":
    main()